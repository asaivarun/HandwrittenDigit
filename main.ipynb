{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAGHAVA\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from beautifultable import BeautifulTable\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.svm import SVC \n",
    "from keras.utils import np_utils\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import fetch_mldata \n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(r):\n",
    "    return np.exp(r)/np.sum(np.exp(r), axis=1).reshape((-1,1))\n",
    "\n",
    "def loss_cross_entropy(y_predicted, y):\n",
    "    return -np.sum(y * np.log(y_predicted))\n",
    "\n",
    "def accuracy(A,Y):\n",
    "    d = A.argmax(axis=1) - Y.argmax(axis=1)\n",
    "    a=(1.0 - (float(np.count_nonzero(d)) / len(d)))\n",
    "    return a*100\n",
    "  \n",
    "def logr(TrainingData,TrainingTarget,ValData,ValDataAct,TestData,TestDataAct,TestData_USPS,TestDataAct_USPS):\n",
    "    table1 = BeautifulTable()\n",
    "    table1.column_headers = [\"Eta\",\"TrA\",\"VaA\",\"TeA\",\"TeA_USPS\"] \n",
    "#     learningRate = [0.0005,0.001,0.005,0.01,0.1]\n",
    "    learningRate = [0.0001]\n",
    "   \n",
    "\n",
    "    for l in learningRate:\n",
    "       \n",
    "        W_Now=np.random.rand(10,784)\n",
    "    #             print(W_Now.shape)\n",
    "    #             print(W_Now)\n",
    "   \n",
    "        for i in range(0,200):\n",
    "            for j in range(0,50000):\n",
    "                    a=softmax(np.dot(TrainingData[j],np.transpose(W_Now)).reshape(-1,10))\n",
    "#                     loss=loss_cross_entropy(a, TrainingTarget[j:j+100])  \n",
    "                    D_E= np.dot(np.transpose(TrainingData[j:j+1]),a-TrainingTarget[j])\n",
    "                    W_Now = W_Now - l*np.transpose(D_E)\n",
    "\n",
    "#             #-----------------Training Data Accuracy---------------------#\n",
    "        a_tr=softmax(np.dot(TrainingData,np.transpose(W_Now)))\n",
    "        TrA=accuracy(a_tr,TrainingTarget)\n",
    "#         Trloss=loss_cross_entropy(a, TrainingTarget) \n",
    "\n",
    "        a_va=softmax(np.dot(ValData,np.transpose(W_Now)))\n",
    "        VaA=accuracy(a_va,ValDataAct)\n",
    "#             #-----------------Testing Data Accuracy---------------------#\n",
    "        a_ts=softmax(np.dot(TestData,np.transpose(W_Now)))\n",
    "        TeA=accuracy(a_ts,TestDataAct)\n",
    "       \n",
    "        a_ts_usps=softmax(np.dot(TestData_USPS,np.transpose(W_Now)))\n",
    "        TeA_USPS=accuracy(a_ts_usps,TestDataAct_USPS)\n",
    "           \n",
    "        table1.append_row([l,float(TrA),float(VaA),float(TeA),float(TeA_USPS)])\n",
    "    return table1,a_tr,a_va,a_ts,a_ts_usps \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-------+----------+\n",
      "| Eta |  TrA  |  VaA  |  TeA  | TeA_USPS |\n",
      "+-----+-------+-------+-------+----------+\n",
      "| 0.0 | 91.97 | 92.21 | 91.79 |  32.897  |\n",
      "+-----+-------+-------+-------+----------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table1,c1_tr,c1_va,c1_ts,c1_ts_usps = logr(training_data[0],np_utils.to_categorical(training_data[1], num_classes = 10),validation_data[0],np_utils.to_categorical(validation_data[1], num_classes = 10),test_data[0],np_utils.to_categorical(test_data[1], num_classes = 10),USPSMat,np_utils.to_categorical(USPSTar, num_classes = 10))\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic SGD Confusion matrix MNIST :n[[ 958    0    0    3    0    3    8    3    5    0]\n",
      " [   0 1110    2    2    0    3    4    1   13    0]\n",
      " [   5   12  907   21    8    4   16   12   41    6]\n",
      " [   3    1   17  920    1   25    2    9   23    9]\n",
      " [   2    3    7    1  910    0   13    3    9   34]\n",
      " [  11    3    7   38   10  753   19    9   34    8]\n",
      " [  11    3    4    2   10   12  910    3    3    0]\n",
      " [   2    6   23    8    7    1    0  946    3   32]\n",
      " [  10    7    6   22    9   34   10   12  861    3]\n",
      " [   9    8    1    8   37    7    1   25    9  904]]\n",
      "Logistic SGD Confusion matrix USPS :n[[ 399    3  251  103  148  367  100   70  116  443]\n",
      " [  70  280  217  267  192  154   28  532  237   23]\n",
      " [ 105   17 1158  191   39  247   75   51   90   26]\n",
      " [  49    3  174 1169   14  441   11   58   58   23]\n",
      " [  44   42   46   69  776  151   64  299  301  208]\n",
      " [  83    8  203  179   37 1234  101   54   82   19]\n",
      " [ 167    6  512   82   69  436  618   15   54   41]\n",
      " [ 147  136  149  597   36  109   16  441  255  114]\n",
      " [ 243   22  158  260   85  680  127   54  283   88]\n",
      " [  29   83  106  524  108   73   18  471  367  221]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(\"Logistic SGD Confusion matrix MNIST :n%s\" % metrics.confusion_matrix(test_data[1], c1_ts.argmax(axis=1)))\n",
    "print(\"Logistic SGD Confusion matrix USPS :n%s\" % metrics.confusion_matrix(USPSTar, c1_ts_usps.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+------+-------+-------+----------+\n",
      "| 1st Layer Hidden Nod | 2nd Layer Hidden Nod | TrA  |  VaA  |  TeA  | TeA_USPS |\n",
      "|          es          |          es          |      |       |       |          |\n",
      "+----------------------+----------------------+------+-------+-------+----------+\n",
      "|         1024         |         2048         | 0.97 | 0.973 | 0.965 |  0.425   |\n",
      "+----------------------+----------------------+------+-------+-------+----------+\n"
     ]
    }
   ],
   "source": [
    "num_classes=10 \n",
    "image_vector_size=28*28 \n",
    "I1=[1024]\n",
    "I2=[2048]\n",
    "table2 = BeautifulTable()\n",
    "table2.column_headers = [\"1st Layer Hidden Nodes\",\"2nd Layer Hidden Nodes\",\"TrA\",\"VaA\",\"TeA\",\"TeA_USPS\"] \n",
    "\n",
    "for i in I1: \n",
    "    for j in I2:\n",
    "        y_train= keras.utils.to_categorical(np.concatenate((training_data[1],validation_data[1]), axis=0), num_classes) \n",
    "        y_test= keras.utils.to_categorical(test_data[1], num_classes) \n",
    "        y_test_usps= keras.utils.to_categorical(USPSTar, num_classes) \n",
    "        x_train=np.concatenate((training_data[0],validation_data[0]), axis=0)\n",
    "\n",
    "        image_size = 784 \n",
    "        model = Sequential() \n",
    "        model.add(Dense(units=i, activation='relu', input_shape=(image_size,))) \n",
    "        model.add(Dense(units=j, activation='sigmoid')) \n",
    "        model.add(Dense(units=num_classes, activation='softmax')) \n",
    "        model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])  \n",
    "        epochs=100\n",
    "        history = model.fit(x_train, y_train, batch_size=128, epochs=epochs, verbose=False,validation_split=.1)  \n",
    "        # %matplotlib inline\n",
    "#             #-----------------TrainingData Accuracy---------------------#\n",
    "        df = pd.DataFrame(history.history)\n",
    "        # df.plot(subplots=True, grid=True, figsize=(10,15))\n",
    "#         loss,accuracy = model.evaluate(test_data[0], y_test, verbose=False) \n",
    "#         loss_usps,accuracy_usps = model.evaluate(np.array(USPSMat),y_test_usps, verbose=False)\n",
    "        NN_ts=model.predict(test_data[0])\n",
    "        NN_ts_usps=model.predict(np.array(USPSMat))\n",
    "#             #-----------------Testing Data Accuracy---------------------#\n",
    "        accuracy=accuracy_score(test_data[1],NN_ts.argmax(axis=1))\n",
    "        accuracy_usps=accuracy_score(USPSTar, predicted_usps.argmax(axis=1))\n",
    "        table2.append_row([i,j,float(df.iloc[epochs-1,3]),float(df.iloc[epochs-1,1]),float(accuracy),float(accuracy_usps)])\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Confusion matrix MNIST :n[[ 966    0    1    2    0    4    4    2    1    0]\n",
      " [   0 1119    4    0    0    1    4    2    5    0]\n",
      " [   5    1 1004    3    3    0    3    7    5    1]\n",
      " [   1    0   10  970    0   10    0   10    6    3]\n",
      " [   1    1    6    0  944    0    7    2    2   19]\n",
      " [   8    1    0    9    2  853    9    1    5    4]\n",
      " [   8    3    0    0    7    8  928    1    3    0]\n",
      " [   1    9   12    4    2    1    0  988    1   10]\n",
      " [   5    2    3    8    4    6    9    8  925    4]\n",
      " [   6    6    1    8   17    5    1    7    3  955]]\n",
      " Confusion matrix USPS :n[[ 569    1  195   72  145  157   52  103  120  586]\n",
      " [  74  379  327  114   87   78   29  760   91   61]\n",
      " [  65    8 1485  107   25  144   62   50   41   12]\n",
      " [  44    3  142 1427    3  261    5   45   60   10]\n",
      " [  23   52   59   33  924  131   21  410  220  127]\n",
      " [ 110    4  159  143   20 1365   51   42   92   14]\n",
      " [ 142    7  448   56   61  273  907   20   46   40]\n",
      " [  66  134  365  444   20  125    6  697  116   27]\n",
      " [ 180   12  223  282   65  550   73   92  471   52]\n",
      " [  20   93  113  408   95   62    3  610  317  279]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(\" Confusion matrix MNIST :n%s\" % metrics.confusion_matrix(test_data[1], NN_ts.argmax(axis=1)))\n",
    "print(\" Confusion matrix USPS :n%s\" % metrics.confusion_matrix(USPSTar, NN_ts_usps.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_n(A,Y):\n",
    "    return (1.0 - (float(np.count_nonzero(A-Y)) / len(A-Y)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-------+------+----------+\n",
      "| Kernel |  gamma  | TrA  |  VaA  | TeA  | TeA_USPS |\n",
      "+--------+---------+------+-------+------+----------+\n",
      "|  rbf   | default | 0.94 | 0.945 | 0.93 |  0.385   |\n",
      "+--------+---------+------+-------+------+----------+\n",
      "SVM Confusion matrix MNIST :n[[ 966    0    1    0    1    5    5    1    1    0]\n",
      " [   0 1118    3    2    0    1    4    1    6    0]\n",
      " [   9    2  934   10   14    2   18   14   27    2]\n",
      " [   2    2   19  932    1   21    1   11   17    4]\n",
      " [   1    4    7    0  919    0   10    3    2   36]\n",
      " [   7    6    5   36    9  795   14    4   11    5]\n",
      " [  10    3    5    1    6   14  918    0    1    0]\n",
      " [   3   17   23    4   10    1    0  939    4   27]\n",
      " [   4    8    8   18    8   28   11    8  873    8]\n",
      " [  10    9    1   12   39    7    1   13    6  911]]\n",
      "SVM Confusion matrix USPS :n[[ 565    4  409   18  336  223   77   37   11  320]\n",
      " [  98  455  240  124  417  190   46  397   19   14]\n",
      " [ 107   23 1345   70   48  213   67   84   31   11]\n",
      " [  71    7  164 1118   15  506    6   63   27   23]\n",
      " [  18   95   62   16 1238  245   16  162   64   84]\n",
      " [ 137   20  188  100   34 1371   69   52   18   11]\n",
      " [ 236    8  441   31  115  377  740   14   13   25]\n",
      " [  49  262  393  245   86  407   28  440   57   33]\n",
      " [ 114   32  193  186  119  927   97   40  249   43]\n",
      " [  33  197  189  255  277  163   13  486  205  182]]\n"
     ]
    }
   ],
   "source": [
    "# SVM  \n",
    "num_classes=10 \n",
    "table3 = BeautifulTable()\n",
    "table3.column_headers = [\"Kernel\",\"gamma\",\"TrA\",\"VaA\",\"TeA\",\"TeA_USPS\"] \n",
    "\n",
    "\n",
    "classifier1 = SVC(kernel='rbf',gamma='auto')\n",
    "classifier1.fit(training_data[0][0:20000], training_data[1][0:20000])  \n",
    "\n",
    "# Predict Test Set\n",
    "\n",
    "Train_Accuracy_SVM=accuracy_score(training_data[1],classifier1.predict(training_data[0]))\n",
    "Validation_Accuracy_SVM=accuracy_score(validation_data[1],classifier1.predict(validation_data[0]))\n",
    "\n",
    "predicted = classifier1.predict(test_data[0])\n",
    "predicted_usps = classifier1.predict(np.array(USPSMat))\n",
    "Test_Accuracy_SVM=accuracy_score(test_data[1],predicted)\n",
    "Test_Accuracy_USPS_SVM=accuracy_score(USPSTar, predicted_usps)\n",
    "table3.append_row(['rbf','default',float(Train_Accuracy_SVM),float(Validation_Accuracy_SVM),float(Test_Accuracy_SVM),float(Test_Accuracy_USPS_SVM)])\n",
    "\n",
    "print(table3)\n",
    "# confusion matrix\n",
    "print(\"SVM Confusion matrix MNIST :n%s\" % metrics.confusion_matrix(test_data[1], predicted))\n",
    "print(\"SVM Confusion matrix USPS :n%s\" % metrics.confusion_matrix(USPSTar, predicted_usps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+-------+------+----------+\n",
      "|                No. of trees                  | TrA |  VaA  | TeA  | TeA_USPS |\n",
      "+----------------------------------------------+-----+-------+------+----------+\n",
      "|                     200                      | 1.0 | 0.974 | 0.97 |  0.401   |\n",
      "+----------------------------------------------+-----+-------+------+----------+\n",
      "RF Confusion matrix MNIST Random Forest:n[[ 969    0    0    0    0    2    3    1    4    1]\n",
      " [   0 1124    2    3    0    2    2    1    1    0]\n",
      " [   6    0  999    7    2    0    4    8    6    0]\n",
      " [   0    0    9  972    0    9    0    9    9    2]\n",
      " [   1    0    1    0  957    0    4    1    3   15]\n",
      " [   2    0    1   15    4  858    5    1    4    2]\n",
      " [   7    3    0    0    2    4  938    0    4    0]\n",
      " [   1    4   16    1    1    0    0  992    1   12]\n",
      " [   3    0    6    8    6    4    5    3  927   12]\n",
      " [   7    5    2    8   14    5    1    5    3  959]]\n",
      "RF Confusion matrix USPS Random Forest:n[[ 649   12  270   60  418  159   61  117    3  251]\n",
      " [  44  566  127  105   43   95   23  977   18    2]\n",
      " [  85   30 1283   81   55  168   18  272    5    2]\n",
      " [  36   10   93 1268   51  326    3  187    5   21]\n",
      " [  12  215   66   22 1061  180   12  378   30   24]\n",
      " [ 129   31  134   90   26 1437   20  121    5    7]\n",
      " [ 309   45  265   27   86  332  780  141    4   11]\n",
      " [  44  344  355  253   35  231   32  693    6    7]\n",
      " [  44   50  156  210   89 1064   74  108  185   20]\n",
      " [  13  260  253  317  236  129   10  598   83  101]]\n"
     ]
    }
   ],
   "source": [
    "#Trees=[1,3,5,10,15,25,50,75,100,200]\n",
    "Trees=[200]\n",
    "table4 = BeautifulTable()\n",
    "table4.column_headers = [\"No. of trees \",\"TrA\",\"VaA\",\"TeA\",\"TeA_USPS\"] \n",
    "\n",
    "for i in Trees:\n",
    "# #RandomForestClassifier  \n",
    "    classifier2 = RandomForestClassifier(n_estimators=i)\n",
    "    classifier2.fit(training_data[0], training_data[1])\n",
    "    \n",
    "    # Predict Test Set\n",
    "    Train_Accuracy_RF=accuracy_score(training_data[1],classifier2.predict(training_data[0]))\n",
    "    Validation_Accuracy_RF=accuracy_score(validation_data[1],classifier2.predict(validation_data[0]))\n",
    "    predicted1 = classifier2.predict(test_data[0])\n",
    "    predicted1_usps = classifier2.predict(np.array(USPSMat))\n",
    "    Test_Accuracy_RF=accuracy_score(test_data[1],predicted1)\n",
    "    Test_Accuracy_USPS_RF=accuracy_score(USPSTar, predicted1_usps)\n",
    "    table4.append_row([i,float(Train_Accuracy_RF),float(Validation_Accuracy_RF),float(Test_Accuracy_RF),float(Test_Accuracy_USPS_RF)])\n",
    "print(table4)\n",
    "    \n",
    "# confusion matrix\n",
    "print(\"RF Confusion matrix MNIST Random Forest:n%s\" % metrics.confusion_matrix(test_data[1], predicted1))\n",
    "print(\"RF Confusion matrix USPS Random Forest:n%s\" % metrics.confusion_matrix(USPSTar, predicted1_usps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(r):\n",
    "    return np.exp(r)/np.sum(np.exp(r), axis=1).reshape((-1,1))\n",
    "\n",
    "def loss_cross_entropy(y_predicted, y):\n",
    "    return -np.sum(y * np.log(y_predicted))\n",
    "\n",
    "def accuracy(A,Y):\n",
    "    d = A.argmax(axis=1) - Y.argmax(axis=1)\n",
    "    a=(1.0 - (float(np.count_nonzero(d)) / len(d)))\n",
    "    return a*100\n",
    "  \n",
    "def logr_mb_sgd(TrainingData,TrainingTarget,ValData,ValDataAct,TestData,TestDataAct,TestData_USPS,TestDataAct_USPS):\n",
    "    table5 = BeautifulTable()\n",
    "    table5.column_headers = [\"Eta\",\"TrA\",\"VaA\",\"TeA\",\"TeA_USPS\"] \n",
    "    learningRate = [0.0001]\n",
    "#     learningRate = [0.01]\n",
    "   \n",
    "\n",
    "    for l in learningRate:\n",
    "       \n",
    "        W_Now=np.random.rand(10,784)\n",
    "    #             print(W_Now.shape)\n",
    "    #             print(W_Now)\n",
    "   \n",
    "        for i in range(0,200):\n",
    "            batch_size=64\n",
    "            for j in range(0,50000,batch_size):\n",
    "                    a=softmax(np.dot(TrainingData[j:j+batch_size],np.transpose(W_Now)))\n",
    "#                     loss=loss_cross_entropy(a, TrainingTarget[j:j+100])  \n",
    "                    D_E= np.dot(np.transpose(TrainingData[j:j+batch_size]),a-TrainingTarget[j:j+batch_size])\n",
    "                    W_Now = W_Now - l*np.transpose(D_E)\n",
    "\n",
    "#             #-----------------TrainingData Accuracy---------------------#\n",
    "        a5_tr=softmax(np.dot(TrainingData,np.transpose(W_Now)))\n",
    "        TrA=accuracy(a5_tr,TrainingTarget)\n",
    "#         Trloss=loss_cross_entropy(a, TrainingTarget) \n",
    "\n",
    "        a5_va=softmax(np.dot(ValData,np.transpose(W_Now)))\n",
    "        VaA=accuracy(a5_va,ValDataAct)\n",
    "#             #-----------------Testing Data Accuracy---------------------#\n",
    "\n",
    "        a5_ts=softmax(np.dot(TestData,np.transpose(W_Now)))\n",
    "        TeA=accuracy(a5_ts,TestDataAct)\n",
    "       \n",
    "        a5_ts_usps=softmax(np.dot(TestData_USPS,np.transpose(W_Now)))\n",
    "        TeA_USPS=accuracy(a5_ts_usps,TestDataAct_USPS)\n",
    "           \n",
    "        table5.append_row([l,float(TrA),float(VaA),float(TeA),float(TeA_USPS)])\n",
    "    return table5,a5_tr,a5_va,a5_ts,a5_ts_usps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-------+----------+\n",
      "| Eta |  TrA   |  VaA  |  TeA  | TeA_USPS |\n",
      "+-----+--------+-------+-------+----------+\n",
      "| 0.0 | 91.952 | 92.37 | 91.94 |  32.482  |\n",
      "+-----+--------+-------+-------+----------+\n",
      "Logistic SGD Confusion matrix MNIST :n[[ 956    0    1    2    0    7   11    1    2    0]\n",
      " [   0 1111    2    2    0    2    4    1   13    0]\n",
      " [   8    9  914   18    9    1   12   11   41    9]\n",
      " [   1    0   18  916    0   29    2   12   25    7]\n",
      " [   2    3    4    2  919    0    7    2    7   36]\n",
      " [  10    3    5   43   10  756   16    6   37    6]\n",
      " [   8    3    5    2   10   17  909    2    2    0]\n",
      " [   2    5   20   10   10    0    0  947    6   28]\n",
      " [   7    7    9   23    9   31    9   14  861    4]\n",
      " [  10    6    1   10   38    7    0   27    5  905]]\n",
      "Logistic SGD Confusion matrix USPS :n[[ 478    2  324   94  119  293   94  156  205  235]\n",
      " [  85  266  289  191  208  105   20  594  222   20]\n",
      " [ 131   11 1266  128   30  177   75   63   94   24]\n",
      " [  69    3  247 1068   17  417    7   86   66   20]\n",
      " [  50   41   64   54  772  154   73  321  293  178]\n",
      " [  76   11  307  167   32 1150   75   65  103   14]\n",
      " [ 184    5  528   92   61  457  599   12   39   23]\n",
      " [ 154  111  187  608   67  133   10  409  253   68]\n",
      " [ 243   19  137  299  127  662  106   55  290   62]\n",
      " [  25   87   95  543  124  128   11  455  334  198]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table5,c5_tr,c5_va,c5_ts,c5_ts_usps= logr_mb_sgd(training_data[0],np_utils.to_categorical(training_data[1], num_classes = 10),validation_data[0],np_utils.to_categorical(validation_data[1], num_classes = 10),test_data[0],np_utils.to_categorical(test_data[1], num_classes = 10),USPSMat,np_utils.to_categorical(USPSTar, num_classes = 10))\n",
    "print(table5)\n",
    "# confusion matrix\n",
    "print(\"Logistic SGD Confusion matrix MNIST :n%s\" % metrics.confusion_matrix(test_data[1], c5_ts.argmax(axis=1)))\n",
    "print(\"Logistic SGD Confusion matrix USPS :n%s\" % metrics.confusion_matrix(USPSTar, c5_ts_usps.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods - Majority/Hard Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Voting MNIST Test accuracy is 0.9675\n",
      "Majority Voting USPS Test accuracy is 0.4224211210560528\n",
      "Majority_Voting Confusion matrix MNIST :n[[ 968    0    1    1    0    3    4    1    2    0]\n",
      " [   0 1123    2    2    0    1    3    1    3    0]\n",
      " [   7    1  997    5    3    0    3    8    7    1]\n",
      " [   0    0    9  973    0    7    0   11    8    2]\n",
      " [   1    0    3    0  955    0    6    2    2   13]\n",
      " [   7    1    0   12    2  854    7    1    6    2]\n",
      " [   7    3    0    0    4    8  933    1    2    0]\n",
      " [   1    7   15    2    2    1    0  989    0   11]\n",
      " [   4    1    4    9    5    5    8    8  926    4]\n",
      " [   8    6    1    9   14    3    1    7    3  957]]\n",
      "Majority_Voting Confusion matrix USPS :n[[ 586    2  245   69  209  191   57  103   78  460]\n",
      " [  65  413  270  114   56   87   24  851   83   37]\n",
      " [  77   10 1478   93   29  149   43   86   28    6]\n",
      " [  38    3  131 1411    4  290    4   63   48    8]\n",
      " [  17   77   56   22  991  150   17  367  202  101]\n",
      " [  99   11  181  124   15 1418   37   51   56    8]\n",
      " [ 172    9  423   48   55  325  886   26   28   28]\n",
      " [  61  190  324  441   24  164   10  667   97   22]\n",
      " [ 136   18  180  274   67  750   74   93  365   43]\n",
      " [  17  124  117  413  116   78    5  596  301  233]]\n"
     ]
    }
   ],
   "source": [
    "Classifier1_ts=c1_ts\n",
    "Classifier1_ts_usps= c1_ts_usps\n",
    "Classifier2_ts=NN_ts\n",
    "Classifier2_ts_usps=NN_ts_usps\n",
    "Classifier3_ts=np_utils.to_categorical(predicted, num_classes = 10)\n",
    "Classifier3_ts_usps=np_utils.to_categorical(predicted_usps, num_classes = 10)\n",
    "Classifier4_ts=np_utils.to_categorical(predicted1, num_classes = 10)\n",
    "Classifier4_ts_usps=np_utils.to_categorical(predicted1_usps, num_classes = 10)\n",
    "Classifier5_ts=c5_ts\n",
    "Classifier5_ts_usps=c5_ts_usps\n",
    "\n",
    "#Logic for Majority/ Hard Voting\n",
    "Classifier_test=Classifier1_ts+Classifier2_ts+Classifier3_ts+Classifier4_ts+Classifier5_ts\n",
    "Classifier_test_usps=Classifier1_ts_usps+Classifier2_ts_usps+Classifier3_ts_usps+Classifier4_ts_usps+Classifier5_ts_usps\n",
    "\n",
    "\n",
    "Majority_Voting_test = accuracy_score(test_data[1],Classifier_test.argmax(axis=1))\n",
    "Majority_Voting_test_usps= accuracy_score(USPSTar,Classifier_test_usps.argmax(axis=1))\n",
    "print(\"Majority Voting MNIST Test accuracy is \" + str(Majority_Voting_test))\n",
    "print(\"Majority Voting USPS Test accuracy is \" +str(Majority_Voting_test_usps))\n",
    "\n",
    "# confusion matrix\n",
    "print(\"Majority_Voting Confusion matrix MNIST :n%s\" % metrics.confusion_matrix(test_data[1], Classifier_test.argmax(axis=1)))\n",
    "print(\"Majority_Voting Confusion matrix USPS :n%s\" % metrics.confusion_matrix(USPSTar, Classifier_test_usps.argmax(axis=1)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
